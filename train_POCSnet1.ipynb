{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from model_structures import *\n",
    "from data_loader import *\n",
    "from loss_func import *\n",
    "import datetime\n",
    "import time\n",
    "from custom_layers import *\n",
    "from keras.optimizers import Adam\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {}\n",
    "\n",
    "opt['train_path'] = '/home/sylar/data/Simu/bg_rm_simu_train/brain_phantom_simu/patch_simu_hemr_4/patch_calculated_msk'\n",
    "opt['train_img1_path'] = ['/msk_arr/msk']\n",
    "opt['train_img2_path'] = ['/phs_total/phs']\n",
    "opt['train_label_path'] = ['/phs_tissue/phs']\n",
    "\n",
    "opt['reso'] = (0.9375,0.9375,1.5)\n",
    "\n",
    "opt['rad'] = [5,3]\n",
    "opt['ker'] = []\n",
    "for n in range(len(opt['rad'])):\n",
    "    nx,ny,nz=round(opt['rad'][n]/opt['reso'][0]),round(opt['rad'][n]/opt['reso'][1]),round(opt['rad'][n]/opt['reso'][2])\n",
    "    nx,ny,nz = max(nx,2),max(ny,2),max(nz,2)\n",
    "    ky,kx,kz = np.mgrid[-nx:nx+1,-ny:ny+1,-nz:nz+1]\n",
    "    k = (kx**2/nx**2+ky**2/ny**2+kz**2/nz**2<=1)\n",
    "    a = k/np.sum(k)\n",
    "    opt['ker'].append(a)\n",
    "\n",
    "opt['patch_size'] = (48,48,48)\n",
    "\n",
    "opt['lbd0'] = 100   \n",
    "opt['lbd1'] = 0.1\n",
    "opt['thr'] = 0.3\n",
    "opt['iter'] = 5\n",
    "\n",
    "opt['batch_size'] = 30\n",
    "opt['channels'] = len(opt['rad'])+1\n",
    "opt['img_shape'] = opt['patch_size'] + (opt['channels'],)\n",
    "opt['in_shape'] = opt['patch_size'] + (1,)\n",
    "opt['rnd_crop'] = False\n",
    "opt['is_aug'] = False\n",
    "\n",
    "opt['model_restored'] = False\n",
    "opt['model_restored_epoch'] = 10\n",
    "opt['model_total_epoch'] = 150\n",
    "opt['model_save_interval'] = 1\n",
    "\n",
    "opt['learning_rate'] = 2e-5\n",
    "opt['beta_1'] = 0.9\n",
    "opt['beta_2'] = 0.99\n",
    "\n",
    "opt['loss'] = mse\n",
    "\n",
    "opt['display_nums'] = [10,20,30]\n",
    "\n",
    "opt['model_save_path'] = '/home/sylar/data/results/model1_revision/pocsnet1_model'\n",
    "opt['checkpoint_path'] = opt['model_save_path']+\"/cp{epoch}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt['model_restored'] == False:\n",
    "    index = list(range(1,10001))\n",
    "    random.shuffle(index)\n",
    "    opt['train_index'] = index[0:9500]\n",
    "    opt['val_index'] = index[9500:10000]\n",
    "    np.save(opt['model_save_path'] + '/train_index8.npy', opt['train_index'])\n",
    "    np.save(opt['model_save_path'] + '/val_index2.npy', opt['val_index'])\n",
    "else:\n",
    "    opt['train_index'] = np.load(opt['model_save_path']+'/train_index8.npy')\n",
    "    opt['val_index'] = np.load(opt['model_save_path']+'/val_index2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt['train_data'] = Data_loaders_bgrm(opt)\n",
    "x1_train, x2_train, y_train = opt['train_data'].next([1],opt)\n",
    "\n",
    "print('training data size: '+ str(opt['train_data'].data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loading data preview'''\n",
    "ch,cw,cd = findcenter3d(y_train[0])\n",
    "aa=cw\n",
    "dd=cd\n",
    "\n",
    "f, axarr = plt.subplots(2, 3, figsize=(20, 10))\n",
    "axarr[1,0].imshow(np.flip(np.transpose(x1_train[0,:,aa,:,1].squeeze()),0))\n",
    "axarr[1,0].axis('off')\n",
    "axarr[0,2].imshow(np.transpose(y_train[0,:,:,dd].squeeze()))\n",
    "axarr[0,2].axis('off')\n",
    "axarr[0,0].imshow(np.transpose(x1_train[0,:,:,dd,1].squeeze()))\n",
    "axarr[0,0].axis('off')\n",
    "axarr[1,1].imshow(np.flip(np.transpose(x2_train[0,:,aa,:].squeeze()),0))\n",
    "axarr[1,1].axis('off')\n",
    "axarr[1,2].imshow(np.flip(np.transpose(y_train[0,:,aa,:].squeeze()),0))\n",
    "axarr[1,2].axis('off')\n",
    "axarr[0,1].imshow(np.transpose(x2_train[0,:,:,dd].squeeze()))\n",
    "axarr[0,1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "opt['c_iter'] = 5\n",
    "smv,trun = smv_iter_array(opt)\n",
    "model = {}\n",
    "\n",
    "model['vnet'] = unet_at1(opt, 1)\n",
    "\n",
    "x = Input(shape=opt['img_shape'])\n",
    "LP = tf.zeros_like(x[...,1:2])\n",
    "\n",
    "model['CG_grad'] = cg_br_grad_model(x,LP,smv,trun,opt)\n",
    "\n",
    "LP = model['CG_grad'](LP)\n",
    "for it in range(opt['iter']):\n",
    "    net_out = model['vnet'](LP)\n",
    "    LP = model['CG_grad'](net_out)\n",
    "\n",
    "model['optimizer'] = Adam(opt['learning_rate'], opt['beta_1'], opt['beta_2'])\n",
    "\n",
    "model['combine'] = Model(inputs = x,\n",
    "                     outputs = [net_out, LP],\n",
    "                     name='combine_model')\n",
    "\n",
    "model['combine'].compile(optimizer = model['optimizer'],\n",
    "                     loss = [opt['loss'],opt['loss']],\n",
    "                     loss_weights = [0.5, 0.5],\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "if opt['model_restored'] == True:\n",
    "    model['vnet'].load_weights(opt['checkpoint_path'].format(epoch = opt['model_restored_epoch']))\n",
    "    start_epoch = opt['model_restored_epoch']\n",
    "else:\n",
    "    start_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr_train_im = opt['train_data'].data_size\n",
    "nr_im_per_epoch = int(np.ceil(nr_train_im / opt['batch_size']) * opt['batch_size'])\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Start Time:\" + str(start_time))\n",
    "\n",
    "avg_epoch_cost = []\n",
    "print('-----Start Training-----')\n",
    "for epoch in range(start_epoch, opt['model_total_epoch']):\n",
    " \n",
    "    order = np.concatenate((np.random.permutation(nr_train_im),\n",
    "                                         np.random.randint(nr_train_im, size=nr_im_per_epoch - nr_train_im)))\n",
    "    avg_img_cost = []\n",
    "    for block_i in range(1, nr_im_per_epoch+1, opt['batch_size']):\n",
    "        index = order[block_i:block_i+opt['batch_size']]\n",
    "        x1_train, x2_train, y_train = opt['train_data'].next(index, opt)\n",
    "\n",
    "        # Training\n",
    "        x_in = np.concatenate((x1_train, x2_train),axis=-1)\n",
    "\n",
    "        history = model['combine'].train_on_batch(x_in, [y_train,y_train] )\n",
    "        [y_pred1, y_pred2] = model['combine'].predict(x_in)\n",
    "        m_loss1 = history[0]\n",
    "        m_loss2 = history[1]\n",
    "        m_loss3 = history[2]\n",
    "        norm_loss = np.linalg.norm(y_pred1-y_train)/np.linalg.norm(y_train)\n",
    "        avg_img_cost.append(m_loss1)\n",
    "        if block_i % (30 * opt['batch_size'])==1:\n",
    "            # Plot the progress\n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [Model loss: %f-%f-%f ; nrmse: %f]\" % (epoch, opt['model_total_epoch'],\n",
    "                                                                block_i, nr_train_im,\n",
    "                                                                m_loss1,m_loss2,m_loss3, norm_loss))\n",
    "            display_slice(opt['display_nums'], y_pred1,y_pred2, y_train)\n",
    "    avg_epoch_cost.append(np.mean(avg_img_cost)) \n",
    "                                \n",
    "    print(\"Epoch:\", '%04d' % (epoch), \"Training_cost=\", \"{:.5f}\".format(avg_epoch_cost[-1]))\n",
    "    display_error(range(start_epoch,epoch+1),avg_epoch_cost)\n",
    "\n",
    "    # If at save interval => save models\n",
    "    if epoch % opt['model_save_interval'] == 0:\n",
    "        model['vnet'].save_weights(opt['checkpoint_path'].format(epoch=epoch))\n",
    "            \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Total Time:\" + str(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
